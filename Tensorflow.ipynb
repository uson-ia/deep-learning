{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probando Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST (beginners)\n",
    "\n",
    "Script para descargar los datos de MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que los datos se dividen en 2 matrices, una de datos para entrenar (mnist.train) y otra para probar nuestro\n",
    "aprendizaje (mnist.test).\n",
    "La matriz train cuenta con 55000 imagenes de 28x28 pixeles, es decir, 55000 vectores de tamano 784 que representan\n",
    "imagenes de digitos escritos a mano. La matriz `mnist.train.label` es la que indica que digito es cada imagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de la matriz de imagenes >>  (55000, 784)\n",
      "Dimension de la matriz de labels >>  (55000, 10)\n",
      "Dimensiones de la matriz de prueba >>  (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "print \"Dimensiones de la matriz de imagenes >> \", mnist.train.images.shape\n",
    "print \"Dimension de la matriz de labels >> \", mnist.train.labels.shape\n",
    "print \"Dimensiones de la matriz de prueba >> \", mnist.test.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Presicion con nuestros datos de entrenamiento >>  0.9114\n"
     ]
    }
   ],
   "source": [
    "# x es solo un placeholder, es decir no tiene un valor especifico aun\n",
    "x = tf.placeholder(\"float\", [None, 784])   # La dimension de x, es (lo que sea, 784)\n",
    "W = tf.Variable(tf.zeros([784,10]))        # Pesos\n",
    "b = tf.Variable(tf.zeros([10]))            # Bias\n",
    "y = tf.nn.softmax(tf.matmul(x,W) + b)      # Predicted probability\n",
    "\n",
    "y_ = tf.placeholder(\"float\", [None,10]) \n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)    # Minimizamos con descenso de\n",
    "# gradiente, con un grado de aprendizaje de 0.01\n",
    "\n",
    "# Inicializamos las variables que creamos\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Entrenamos con 1000 iteraciones\n",
    "for i in range(1000):\n",
    "  batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    \n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))    # Acertamos ?\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))   # Con que exactitud ?\n",
    "\n",
    "print \"Presicion con nuestros datos de entrenamiento >> \", sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Deep MINST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision con nuestros datos de prueba >>  0.9142\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow utiliza un backend en C++ para hacer el computa mas eficiente.\n",
    "# Para establecer una coneccion con este backend usaremos una sesion.\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "\n",
    "# Nodos para las imagenes de entrada y las clases de salida objetivo\n",
    "x = tf.placeholder(\"float\", shape=[None, 784])\n",
    "y_ = tf.placeholder(\"float\", shape=[None, 10])\n",
    "\n",
    "# pesos y bias ('tensors' de puros ceros hasta ahora)\n",
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# inicializamos las variables previas definidas para poder usarlas\n",
    "# se inician dentro de la sesion que creamos\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "# ahora implementamos nuestro modelo de regresion (softmax)\n",
    "y = tf.nn.softmax(tf.matmul(x,W) + b)\n",
    "\n",
    "# especificamos nuestra funcion costo a minimizar, la cual sera\n",
    "# 'cross-entropy' entre los objetivos y nuestra prediccion de modelo\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "\n",
    "# usamos descenso de gradiente para minimizar la entropia y repetimos\n",
    "# esto con un epoc de 1000\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "for i in range(1000):\n",
    "  batch = mnist.train.next_batch(50)\n",
    "  train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "    \n",
    "# checamos que tan bien le fue a nuestro modelo comparando\n",
    "# la etiqueta que nuestro modelo dice es la mas probable para cada entrada\n",
    "# con la etiqueta correcta.\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "\n",
    "# como 'correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))' nos\n",
    "# dara una lista de booleanos, convertiremos la lista a 0's y 1's y \n",
    "# despues le sacamos la media\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "# ahora evaluamos nuestra precision con los datos de prueba\n",
    "print \"Precision con nuestros datos de prueba >> \", accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Multilayer Convolutional Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# esta funcion regresa una matriz con pesos iniciales\n",
    "# aleatorios provenientes de una distribucion normal\n",
    "# con desviacion estandar = .1\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# esta funcion regresa una matriz con todos los elementos\n",
    "# igual a .1 y dimensiones = shape\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos nuestras funciones de convolucion y pooling maximo de bloques de 2x2. Las usaremos mas adelante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primera capa convolucional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta capa se procesan 32 caracteristicas por cada pieza de datos de 5x5.\n",
    "\n",
    "Tambien creamos un vector de bias por cada salida (de las 32). Para esto usamos las funciones creadas anteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le hacemos reshape a x, haciendo un tensor 4d. La segunda y tercera dimension corresponden al ancho y alto de la imagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_image = tf.reshape(x, [-1,28,28,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se aplica la funcion rectificadora al vector x convolucionado con el vector de pesos, al mismo tiempo se le anade el ruido.\n",
    "\n",
    "Despues aplicamos el max pooling sobre una region de 2x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creando otra capa mas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agregamos otra capa mas de convolucion para crear la red profunda.\n",
    "\n",
    "Esta capa tendra 64 caracteristicas por cada bloque de 5x5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-29-3beaa269bba2>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-29-3beaa269bba2>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    Ahora anadimos creamos una capota completamente conectada, con 1024 neuronas que procesaran toda la imagen\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Ahora anadimos creamos una capota completamente conectada, con 1024 neuronas que procesaran toda la imagen"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
