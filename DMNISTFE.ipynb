{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Deep MNIST para Expertos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow es una poderosa libreria para hacer calculos numericos a gran escala. Una de las tareas que realiza esta libreria es implemntar y entrenar deep neural networks. En este tutorial se aprende los bloques basicos para construir un modelo TensorFlow mientras se construye un clasificador deep convolutional MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Preparacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de crear el modelo primero se debe cargar el dataset MNIST y despues iniciar una sesion TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Cargar la informacion de MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cargar la informacion se incluye un script que automaticamente descarga e importa el dataset de MNIST. Cuando se ejecuta este script se crea un directorio con en nombre de 'MNIST_data' en el cual se guardan los archivos con los datos.\n",
    "\n",
    "a continuacion se presentan las lineas que ejecutan dicho script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable mnist contiene los arreglos de tipo numpy de entrenamiento, validacion y testing. Ademas provee una funcion que permite iterar atraves de los minibatches de datos los cuales se utilizan mas adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Comenzar una sesion interactiva en TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow se basa en un eficiente backend de C++ para hacer los calculos. La conexion para este backend se llama sesion. El uso mas comun para programas TensorFlow es primero crear un grafo y despues iniciar una sesion.\n",
    "\n",
    "Aqui se utiliza por conveniciencia la clase InteractiveSession la cual hace que TensorFlow sea aun mas flexible acerca de la estructura del codigo Tambien permite intercalar operaciones las cuales construyen un grafo de calculos con el cual se ejecutara el grafo. Esto es particularmente conveniente cuando se trabaja con contextos interactivos como iPython. \n",
    "\n",
    "A continuacion se inicia la sesion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Computacion Grafica (Cambiar este nombre!) grafo computacional .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar calculos numericos eficientes en python usualmente se utilizan librerias como NumPy que hacen operaciones costosas como las multiplicacion de matrices fuera de Python utilizando codigo eficiente implementado en otro lenguaje. Desafortunadamente existe una gran cantidad de overhead al cambiar de una operacion a otra.\n",
    "\n",
    "TensorFlow utiliza librerias implementadas en otros lenguajes pero evitando el overhead. Provee un grafo con operaciones interactivas las cuales corren fuera de python.\n",
    "\n",
    "El rol de codigo en python es construir un grafo externo y dictarle que partes de el grafo se van a utilizar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construir el Modelo de la Regresion Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta seccion se construye el modelo de la regresion softmax con una sola capa. En la siguiente seccion se extiene el caso de la regresion softmax con multicapas en una convolutional network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Placeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comenzar a construir un grafo computacional se crean nodos para las imagenes de entrada y salidas para las clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\", shape=[None, 784])\n",
    "y_ = tf.placeholder(\"float\", shape=[None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui x e y_ no tienen valores especificos ya que son placeholders esto significa que se les asignara un valor de entrada cuando se le pida a TensorFlow ejecutar alguna operacion.\n",
    "\n",
    "Las imagenes de entrada x consisten de un tensor 2d de numeros de punto flotante. Aqui se les asigna un tamano de [None, 784], en donde 784 es la dimension de una imagen de MNIST y None indica que la primera dimension corresponde al tamano del batch este puede tener cualquier tamano. La salida de las clases es -_ el cual consiste de un tensor 2d donde cada renglon es un vector \"one-hot 10-dimensional\" indicando cual es el digito de la clase a la cual corresponde la imagen a tratar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se definen los pesos W y los bias B para el modelo. Se pueden pensar que son entradas adicionales, pero TensorFlow tiene una manera de manejar este tipo de entradas como variables. Una variable es un valor que vive en el grafo computacional de TensorFlow . Estas pueden ser usadas e incluso modificadas por calculos. Comunmente en aplicaciones de machine learning los modelos utilizan parametros variables.\n",
    "\n",
    "A continuacion se declaran dichas bariables para el peso y el bias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero se pasa el valor inicial para cada parametro en la llamada a la funcion tf.variable. En este caso se inicializan w y b con tensores llenos de 0's. W es una matriz de 784 x 10 (Porque se tienen 784 entradas y 10 salidas) y b es un vector de dimension 10 (Porque se tienen 10 clases).\n",
    "\n",
    "Antes de que las variables se utilicen dentro de una sesion se deben inicializar usando esa sesion. Este paso toma los valores iniciales (En este caso los tensores llenos de 0's ) e inicializa todas las variables a la vez\n",
    "\n",
    "A continuacion se inicializan las variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Prediciendo la clase y la funcion de costo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se puede implementar el modelo de regresion. Solo toma una linea de codigo! Se multiplica el vector de imagenes de entrada x por la matriz de pesos W y se agrega el bias b con esto se ejecutan las probabilidades de softmax asignandola a cada clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.nn.softmax(tf.matmul(x,W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funcion de costo se minimiza durante el entrenamiento. La funcion de costo a utilizar es cross-entropy  la cual se utiliza la salida conocida y la de la prediccion del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como nota tf.reduce_sum suma todas las imagenes en el minibatch asi como todas las clases. Se calcula la funcion de costo para todo el minibatch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Entrenando el Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya que se ha definido el modelo y la funcion de costo para entrenamiento se pasa a entrenar el modelo con TensorFlow. TensorFlow cuenta con el grafo computacional el cual se puede usar para encontrar el gradiente del costo respecto a las variables. TensorFlow cuenta con varios algoritmos de optimizacion. Como ejemplo se utiliza el descenso de gradiente con un paso de longitud de 0.01 para minimizar la funcion de costo cross entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que hizo TensorFlow en una sola linea es agregar nuevas operaciones al grafo computacional. Estas operaciones  incluyen la operacion de calcular el gradiente, actualizar los pasos y aplicar los pasos actualizados a los parametros.\n",
    "\n",
    "Cuando se ejecuta trains_step se aplicara descenso de gradiente y actualizara los parametros. Por lo tanto se puede entrenar el modelo ejecutando varias veces train_step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "  batch = mnist.train.next_batch(50)\n",
    "  train_step.run(feed_dict={x: batch[0], y_: batch[1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada iteracion del entrenamiento carga 50 ejemplos de entrenamiento. Cuando se ejecuta la operacion train_step se utiliza feed_dict esto para reemplazar los placeholder de los tensores x e y_ con los ejemplos de entrenamiento. Como nota se puede reemplazar cualquier tensor en el grafo computacional utilizando feed_dict no esta restringido a solo placeholders. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Evaluando el Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que tan bien lo hizo el modelo?\n",
    "\n",
    "Primero se va a revisar si el modelo predijo la etiqueta correcta. tf.argmax es una funcion muy util que regresa el indice mas alto de una posicion en el tensor sobre un eje. Por ejemplo tf.argmax(y, 1) es la etiqueta que se considera mas probable en el modelo mientras que tf.argmax(y_, 1) es la etiqueta correcta. Se utiliza tf.equal para revisar si la prediccion concuerta con la verdadera etiqueta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La instruccion anterior regresa una lista de valores bool. Para determinar que fraccion de datos son correctos se aplica un cast a la lista cambiando los valores bool a valores de punto flotante y luego se calcula la media. Por ejemplo supongamos que tenemos la siguiente lista [True, Flase, True, True] aplicando el cast se convierte en la siguiente lista [1, 0, 1, 1] y despues se saca la media que en este caso es 0.75.\n",
    "\n",
    "A continuacion se presenta lo antes descrito por la siguiente linea de codigo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente se puede evaluar la exactitud del modelo en los datos que se van a probar. El valor obtenido se debe de encontrar entre el 91% de exactitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9092\n"
     ]
    }
   ],
   "source": [
    "print(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Construir una Computational Network Multicapas (reepensar titulo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtener una exactitud de 91% en los datos de MNIST es una mala exactitud en esta se seccion se utiliza un modelo mas sofisticado al cual se le llama convolutional neural network. Con este modelo se obtiene una exactitud alrededor del 99.2% de exactitud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Inicializacion de los pesos (Le falta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para crear este modelo se necesitan muchos pesos y bias. A continuacion se presentan dos funciones utiles que inicializan los pesos y bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Convolucion y Pooling (Le falta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow tambien provee mucha flexibilidad en las operaciones de convolucion y pooling. Como se manejan los limites? cual es el tamanio del paso? En este ejemplo se utiliza la version vanilla que significa que es la version simple. Las Convoluciones utilizan un paso a la vez y zero padded por lo que la salida es del mismo tamanio que la entrada. El pooling maneja maximo bloques de 2 x 2. Para mantener el codigo limpio se crean las siguientes funciones que realizan las operaciones antes mencionadas.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Primer Capa Convolucional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se puede implementar la primer capa la cual consite de la convolucion seguida del max pooling. La convolucion calcula 32 caracteristicas para cada parte de 5 x 5. El peso del tensor tendra la forma de [5, 5, 1, 32]. Las primeras dos dimensiones es el tamano de la parte, el siguiente numebro representa los canales de entrada y el ultimo numero los canales de salida. Se tendra un vector del bias para un componente para cada salida del canal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para aplicar la capa primero se debe de redimensionar x a un tensor de dimension 4 en donde la segunda y tercera dimension corresponden al anchura y altura de la imagen el ultimo numero corresponde al color de los canales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_image = tf.reshape(x, [-1,28,28,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se convoluciona x_image con el tensor de peso, se agrega el bias, se aplica la funcion RELU y finalmente mas pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Segunda Capa Convolucional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para mantener un orden para construir una deep network se apilan varias capas de este tipo. La segunda capa consiste de 64 caracteristicas para cada parte de 5 x 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Capa Densamente Conectada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora el tamanio de las imagenes se han reducido a 7 x 7 Se agrega una capa totalmente conectada con 1024 neuronas las cuales procesan toda la imagen. Se redimensiona el tensor de la capa de pooling a un batch de vectores y se multiplica por la matriz de pesos ademas se agrega el bias y por ultimo se aplica la funcion ReLu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para reducir el sobreajuste se aplicara dropout antes del readout de la capa. Se crea un placeholder para la probabilidad de la salida de la neurona la cual se mantiene durante el dropout. Esto permite activar el dropout durante el entrenamiento y desactivarlo durante el testing. TensorFlow's tf.nn.dropout automaticamente maneja las salidas de las neuronas ademas les agrega una mascara entonces dropout funciona bien sin ningun otro ajuste adicional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Capa de Readout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente se agrega la capa de softmax tal como la que se presento anteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Entrenar y Evaluar el Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que tan bien lo hace el modelo? Para entrenar y evaluar el modelo se usa un codigo parecido al de la capa softmax. Las diferencias son que se reemplaza el algoritmo de optimizacion de descenso de gradiente por uno mas sofisticado el cual se llama ADAM ademas se incluira el parametro adicional keep_prob en feed_dict para controlar la taza de dropout y despues se imprime un mensaje cada 100 iteraciones para ver la exactitud del proceso de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.04\n",
      "step 100, training accuracy 0.8\n",
      "step 200, training accuracy 0.92\n",
      "step 300, training accuracy 0.92\n",
      "step 400, training accuracy 0.96\n",
      "step 500, training accuracy 0.86\n",
      "step 600, training accuracy 0.88\n",
      "step 700, training accuracy 0.92\n",
      "step 800, training accuracy 0.96\n",
      "step 900, training accuracy 1\n",
      "step 1000, training accuracy 1\n",
      "step 1100, training accuracy 0.98\n",
      "step 1200, training accuracy 0.96\n",
      "step 1300, training accuracy 0.98\n",
      "step 1400, training accuracy 0.98\n",
      "step 1500, training accuracy 0.94\n",
      "step 1600, training accuracy 0.92\n",
      "step 1700, training accuracy 0.88\n",
      "step 1800, training accuracy 0.98\n",
      "step 1900, training accuracy 1\n",
      "step 2000, training accuracy 0.98\n",
      "step 2100, training accuracy 0.94\n",
      "step 2200, training accuracy 0.92\n",
      "step 2300, training accuracy 1\n",
      "step 2400, training accuracy 0.98\n",
      "step 2500, training accuracy 1\n",
      "step 2600, training accuracy 0.98\n",
      "step 2700, training accuracy 1\n",
      "step 2800, training accuracy 1\n",
      "step 2900, training accuracy 1\n",
      "step 3000, training accuracy 0.98\n",
      "step 3100, training accuracy 0.94\n",
      "step 3200, training accuracy 0.98\n",
      "step 3300, training accuracy 0.98\n",
      "step 3400, training accuracy 0.98\n",
      "step 3500, training accuracy 0.96\n",
      "step 3600, training accuracy 1\n",
      "step 3700, training accuracy 1\n",
      "step 3800, training accuracy 0.98\n",
      "step 3900, training accuracy 1\n",
      "step 4000, training accuracy 1\n",
      "step 4100, training accuracy 1\n",
      "step 4200, training accuracy 1\n",
      "step 4300, training accuracy 1\n",
      "step 4400, training accuracy 0.98\n",
      "step 4500, training accuracy 0.98\n",
      "step 4600, training accuracy 1\n",
      "step 4700, training accuracy 0.98\n",
      "step 4800, training accuracy 0.94\n",
      "step 4900, training accuracy 1\n",
      "step 5000, training accuracy 1\n",
      "step 5100, training accuracy 0.98\n",
      "step 5200, training accuracy 1\n",
      "step 5300, training accuracy 0.98\n",
      "step 5400, training accuracy 1\n",
      "step 5500, training accuracy 1\n",
      "step 5600, training accuracy 0.98\n",
      "step 5700, training accuracy 0.98\n",
      "step 5800, training accuracy 0.98\n",
      "step 5900, training accuracy 0.98\n",
      "step 6000, training accuracy 1\n",
      "step 6100, training accuracy 1\n",
      "step 6200, training accuracy 0.98\n",
      "step 6300, training accuracy 0.98\n",
      "step 6400, training accuracy 1\n",
      "step 6500, training accuracy 0.98\n",
      "step 6600, training accuracy 1\n",
      "step 6700, training accuracy 0.98\n",
      "step 6800, training accuracy 1\n",
      "step 6900, training accuracy 1\n",
      "step 7000, training accuracy 1\n",
      "step 7100, training accuracy 0.98\n",
      "step 7200, training accuracy 1\n",
      "step 7300, training accuracy 1\n",
      "step 7400, training accuracy 1\n",
      "step 7500, training accuracy 0.98\n",
      "step 7600, training accuracy 1\n",
      "step 7700, training accuracy 1\n",
      "step 7800, training accuracy 1\n",
      "step 7900, training accuracy 1\n",
      "step 8000, training accuracy 1\n",
      "step 8100, training accuracy 1\n",
      "step 8200, training accuracy 1\n",
      "step 8300, training accuracy 1\n",
      "step 8400, training accuracy 1\n",
      "step 8500, training accuracy 0.98\n",
      "step 8600, training accuracy 1\n",
      "step 8700, training accuracy 1\n",
      "step 8800, training accuracy 1\n",
      "step 8900, training accuracy 1\n",
      "step 9000, training accuracy 1\n",
      "step 9100, training accuracy 1\n",
      "step 9200, training accuracy 1\n",
      "step 9300, training accuracy 1\n",
      "step 9400, training accuracy 1\n",
      "step 9500, training accuracy 1\n",
      "step 9600, training accuracy 1\n",
      "step 9700, training accuracy 1\n",
      "step 9800, training accuracy 1\n",
      "step 9900, training accuracy 1\n",
      "step 10000, training accuracy 1\n",
      "step 10100, training accuracy 1\n",
      "step 10200, training accuracy 0.98\n",
      "step 10300, training accuracy 1\n",
      "step 10400, training accuracy 1\n",
      "step 10500, training accuracy 1\n",
      "step 10600, training accuracy 0.98\n",
      "step 10700, training accuracy 0.98\n",
      "step 10800, training accuracy 0.98\n",
      "step 10900, training accuracy 1\n",
      "step 11000, training accuracy 1\n",
      "step 11100, training accuracy 1\n",
      "step 11200, training accuracy 1\n",
      "step 11300, training accuracy 1\n",
      "step 11400, training accuracy 0.98\n",
      "step 11500, training accuracy 1\n",
      "step 11600, training accuracy 1\n",
      "step 11700, training accuracy 1\n",
      "step 11800, training accuracy 1\n",
      "step 11900, training accuracy 1\n",
      "step 12000, training accuracy 0.96\n",
      "step 12100, training accuracy 1\n",
      "step 12200, training accuracy 1\n",
      "step 12300, training accuracy 1\n",
      "step 12400, training accuracy 1\n",
      "step 12500, training accuracy 1\n",
      "step 12600, training accuracy 1\n",
      "step 12700, training accuracy 1\n",
      "step 12800, training accuracy 1\n",
      "step 12900, training accuracy 0.98\n",
      "step 13000, training accuracy 1\n",
      "step 13100, training accuracy 1\n",
      "step 13200, training accuracy 1\n",
      "step 13300, training accuracy 1\n",
      "step 13400, training accuracy 1\n",
      "step 13500, training accuracy 1\n",
      "step 13600, training accuracy 1\n",
      "step 13700, training accuracy 1\n",
      "step 13800, training accuracy 1\n",
      "step 13900, training accuracy 1\n",
      "step 14000, training accuracy 1\n",
      "step 14100, training accuracy 1\n",
      "step 14200, training accuracy 1\n",
      "step 14300, training accuracy 1\n",
      "step 14400, training accuracy 1\n",
      "step 14500, training accuracy 1\n",
      "step 14600, training accuracy 1\n",
      "step 14700, training accuracy 0.98\n",
      "step 14800, training accuracy 1\n",
      "step 14900, training accuracy 1\n",
      "step 15000, training accuracy 1\n",
      "step 15100, training accuracy 0.98\n",
      "step 15200, training accuracy 1\n",
      "step 15300, training accuracy 1\n",
      "step 15400, training accuracy 1\n",
      "step 15500, training accuracy 1\n",
      "step 15600, training accuracy 1\n",
      "step 15700, training accuracy 1\n",
      "step 15800, training accuracy 0.98\n",
      "step 15900, training accuracy 1\n",
      "step 16000, training accuracy 1\n",
      "step 16100, training accuracy 1\n",
      "step 16200, training accuracy 1\n",
      "step 16300, training accuracy 1\n",
      "step 16400, training accuracy 1\n",
      "step 16500, training accuracy 1\n",
      "step 16600, training accuracy 1\n",
      "step 16700, training accuracy 1\n",
      "step 16800, training accuracy 1\n",
      "step 16900, training accuracy 1\n",
      "step 17000, training accuracy 1\n",
      "step 17100, training accuracy 0.98\n",
      "step 17200, training accuracy 1\n",
      "step 17300, training accuracy 1\n",
      "step 17400, training accuracy 1\n",
      "step 17500, training accuracy 1\n",
      "step 17600, training accuracy 1\n",
      "step 17700, training accuracy 1\n",
      "step 17800, training accuracy 1\n",
      "step 17900, training accuracy 0.98\n",
      "step 18000, training accuracy 1\n",
      "step 18100, training accuracy 1\n",
      "step 18200, training accuracy 1\n",
      "step 18300, training accuracy 1\n",
      "step 18400, training accuracy 1\n",
      "step 18500, training accuracy 1\n",
      "step 18600, training accuracy 1\n",
      "step 18700, training accuracy 1\n",
      "step 18800, training accuracy 1\n",
      "step 18900, training accuracy 1\n",
      "step 19000, training accuracy 1\n",
      "step 19100, training accuracy 1\n",
      "step 19200, training accuracy 1\n",
      "step 19300, training accuracy 0.98\n",
      "step 19400, training accuracy 1\n",
      "step 19500, training accuracy 1\n",
      "step 19600, training accuracy 1\n",
      "step 19700, training accuracy 1\n",
      "step 19800, training accuracy 1\n",
      "step 19900, training accuracy 1\n",
      "test accuracy 0.9921\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "sess.run(tf.initialize_all_variables())\n",
    "for i in range(20000):\n",
    "  batch = mnist.train.next_batch(50)\n",
    "  if i%100 == 0:\n",
    "    train_accuracy = accuracy.eval(feed_dict={\n",
    "        x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "    print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "  train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para finalizar la ejecucion del codigo anterior debe regresar una exactidud de aproximadamente 99.2%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
