{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reversión de redes neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resúmen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las redes neuronales artificiales alimentadas hacia adelante (*feed-forward*) son un modelo de cómputo utilizado en el área de aprendizaje máquna para hacer regresión o clasificación de datos. En este trabajo se explora la posibilidad de revertir el proceso de una red neuronal de este tipo y se presenta la reversión de una red con una sola neurona, de tal manera que al alimentarle una valor de salida la neurona pueda computar un entrada que al ser alimentada hacia adelante, resulte en la salida dada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este trabajo se presentan los resultados de una exploración en progreso sobre la reversión de redes neuronales artificiales. La bibliografía y videografía consiste en el material del telecurso *Learning from Data* de Yaser Abu-Mostafa disponible en `https://work.caltech.edu/telecourse.html`.\n",
    "\n",
    "Se comienza la exploración con las redes neuronales hacia adelante y se aborda el problema de revertir sólo una neurona."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revirtiendo una red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La red neuronal que se explorará será alimentada hacia adelante con la función logística como función de transformación y la red habrá sido entrenada previamente.\n",
    "\n",
    "Debido a que la red neuronal ya ha sido entrenada, el proceso de reversión no debe de ajustar los pesos $w^{l}_{i,j}$ ya que las entradas generadas a partir de una salida dada deben de respetar el hecho de que si son alimentadas hacia adelante, la red dará como resultado la salida dada.\n",
    "\n",
    "El proceso de alimentación hacia adelante puede dividirse en procesos de alimentación capa a capa, es decir, las salidas de las neuronas de la capa $i$ son la entrada del proceso que realizan las neuronas en la capa $i+1$. Esto significa que para el proceso revertido, las salidas de las neuronas en la capa $i+1$ son la entrada del proceso que realizan las neuronas revertidas de la capa $i$.\n",
    "\n",
    "El proceso de una capa en una red neuronal no revertida consiste en distribuír una ponderación de las entradas de la capa en las neuronas, cada neurona va a reducir las ponderaciones correspondiente en un solo valor el cual será evaluado en la función de transformación de la neurona.\n",
    "\n",
    "En el tipo de redes que abordamos, la ponderación y agregación de cada neurona corresponden a la operación de producto punto entre vectores y la función de transformación corresponde a la función logística\n",
    "\n",
    "$$\\theta (s) = \\frac{1}{1+\\exp{(-s)}}$$\n",
    "\n",
    "Debido a que la salida de una capa depende de la salida de las neuronas en esta capa y la salida de una neurona es independiente a la salida de el resto de las neuronas en la misma capa:\n",
    "\n",
    "**Hipótesis (1):** Si se puede revertir una neurona en una red neuronal alimentada hacia adelante y todas las neuronas tienen el mismo mecanismo de agregación y transformación, entonces todas las neuronas en la red neuronal podrán ser revertidas.\n",
    "\n",
    "**Hipótesis (2):** Si el proceso de cada neurona es independiente del resto de las neuronas y el proceso de la red neuronal consiste únicamente en comunicar capas de neuronas de manera secuencial y la hipótesis (1) es cierta, entonces la red neuronal podrá ser revertida.\n",
    "\n",
    "Debido a que las salidas de una red neuronal de este tipo son valores producidos por una función logística $\\theta$ y esta tiene las siguientes propiedades:\n",
    "\n",
    "- $\\theta : \\mathbb{R} \\to [0,1]$\n",
    "- $\\theta$ es monotonamente creciente\n",
    "- $\\lim_{x \\to \\infty}  \\theta (x) = 1$\n",
    "- $\\lim_{x \\to -\\infty} \\theta (x) = 0$\n",
    "\n",
    "las salidas de una capa serán valores en el intervalo cerrado $[0,1]$.\n",
    "\n",
    "Debido a que las salidas de una capa serán valores entre 0 y 1. Las entradas antes de ser ponderadas también lo serán. Esto implica que las entradas de la primer capa de la red neuronal deben cumplir esta restricción, es por ello que se debe asegurar que la red neuronal haya sido entrenada con datos normalizados entre 0 y 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reversión de una neurona"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideremos ahora un ánalisis sobre el proceso que realiza una sola neurona.\n",
    "\n",
    "![caption](neurona.png)\n",
    "\n",
    "La capa en la que yace la neurona recibe $d^{l}$ valores de la capa $l-1$ y estos valores junto con un valor $1$ son representados con el vector $\\mathbf{x}^{T}$ (es un vector renglón), cada una de las entradas tiene asociado un peso, estos y el *bias* se representan con el vector $\\mathbf{w}$ (es un vector columna). Las entradas y los pesos forman una combinación lineal de entradas la cual se calcula como el producto punto $\\mathbf{x}^{T} \\cdot \\mathbf{w}$. El resultado del producto se evalúa en la función $\\theta$ para obtener la salida de la neurona $y$.\n",
    "\n",
    "**(a)** Cuando la red neuronal es alimentada hacia adelante $\\mathbf{x}^{T}$ es dado, $\\mathbf{w}$ se conoce y es parte de la red neuronal y $y$ se calcula.\n",
    "\n",
    "**(b)** Si se quiere revertir el proceso de la neurona, debemos de considerar el problema con $y$ dada y $\\mathcal{x}^{T}$ desconocida (a excepción de la primera entrada del vector, que se sabe de antemano es $1$).\n",
    "\n",
    "En caso de **(a)**:\n",
    "$$\\theta \\left( \\sum^{n}_{i=0} w_{i} \\cdot x_{i} \\right) = y$$\n",
    "\n",
    "En caso de **(b)**:\n",
    "$$\\theta^{-1} \\left( y \\right) = \\sum^{n}_{i=0} w_{i} \\cdot x_{i}$$\n",
    "\n",
    "Ya que $\\theta$ tiene inversa el valor de $\\theta^{-1} \\left( y \\right)$ se conoce cuando $y$ es dada. La función logística inversa es llamada *logit* y se define como\n",
    "\n",
    "$$\\theta^{-1}(y) = -\\log{\\left( \\frac{1}{y} - 1 \\right) }$$\n",
    "\n",
    "**Hipótesis (3):** Una red neuronal alimentada hacia adelante puede ser revertida siempre y cuando la función de transformación tenga inversa.\n",
    "\n",
    "Esto hace que el problema a resolver sea el de encontrar la familia de soluciones a una ecuación lineal con $n$ incógnitas y la restricción para todos los valores de la solución\n",
    "$$0 \\leq x_i \\leq 1$$\n",
    "\n",
    "Ya que esta es una ecuación lineal es posible que se tenga una conjunto de soluciones, sin embargo, debido a la restricción de resultados en el intervalo $[0,1]$ es posible también que se tenga una ecuación sin solución.\n",
    "\n",
    "**Hipótesis (4):** Al revertir una red neuronal alimentada hacia adelante, todas las ecuaciones lineales que se presenten hasta la primer capa (en donde se encuentran las entradas de la red) van a tener al menos una solución.\n",
    "\n",
    "**Hipótesis (5):** Si se asegura que las entradas de la red neuronal fueron normalizadas y tienen valores en el intervalo $[0,1]$, todas las ecuaciones lineales que se presenten al revertir la red neuronal van a tener al menos una solución.\n",
    "\n",
    "Algo que es importante enfatizar es que de momento solo nos interesa generar un solo elemento de la familia de soluciones y este elemento debe de ser elegido de manera aleatoria, para que cada vez que alimentemos hacia atrás una red neuronal, podamos obtener entradas diferentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Algoritmo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sea $y$ un número real en el intervalo $[0,1]$.\n",
    "\n",
    "Sea $\\mathbf{w}$ un vector de dimensión $n+1$ tal que $w_{i} \\in \\mathbb{R}$ $\\forall i \\in \\left\\{ 0, 1, \\dots , n-1, n \\right\\}$.\n",
    "\n",
    "Por definición $x_0 = 1$\n",
    "\n",
    "Para cada entrada $i$ del vector de entradas $\\mathbf{x}$ se encuentra el intervalo en el que puede estar $x_i$ despejando de la ecuación lineal y se elige un valor en el intervalo para $x_i$:\n",
    "1. Se obtiene una cota asignando valores a las $x_j$ con $j>i$ tal que la sumatoria de productos sea mínima.\n",
    "2. Se obtiene otra cota asignando valores a las $x_j$ con $j>i$ tal que la sumatoria de productos sea máxima.\n",
    "3. Se identifica la cota inferior $lo$ y la cota superior $hi$.\n",
    "4. Se elige un valor de manera aleatoria en el intervalo $[lo,hi] \\cap [0,1]$.\n",
    "5. Ahora $x_i$ es conocida y se puede proceder a encontrar los valores de $x_j$ con $j>i$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de transformación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function logistic(s)\n",
    "    1.0 / (1 + exp(-s))\n",
    "end\n",
    "\n",
    "function logit(y)\n",
    "    -log(1/y - 1)\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alimentación hacia adelante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function sendInput(xs, ws)\n",
    "    logistic((xs*ws)[1])\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alimentación hacia atrás"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya que se conoce $\\theta^{-1}$, $y$, $w_0$ y $x_0$ podemos evitar el calculo redundante en los despejes de variables utilizando $\\delta = \\theta^{-1}(y) - w_0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function sendOutput(y, ws)\n",
    "    xs = zeros(1, size(ws)[1])\n",
    "    δ  = logit(y) - ws[1]\n",
    "    \n",
    "    xs[1] = 1.0\n",
    "    \n",
    "    negWeight = 0.0 # the sum of all the negative weight\n",
    "    posWeight = 0.0 # the sum of all the possitive weight\n",
    "    for w in ws[2:end]\n",
    "        w < 0.0 ? negWeight += w : posWeight += w\n",
    "    end\n",
    "    \n",
    "    knownVals = 0.0\n",
    "    for i in eachindex(ws)[2:end]\n",
    "        w = ws[i]\n",
    "        w < 0.0 ? negWeight-=w : posWeight-=w\n",
    "        divCoef = 1.0/w\n",
    "        boundA = (δ - negWeight - knownVals)*divCoef\n",
    "        boundB = (δ - posWeight - knownVals)*divCoef\n",
    "        lo, hi = validInterval(boundA, boundB)\n",
    "        x = randInterval(lo, hi)\n",
    "        xs[i] = x\n",
    "        knownVals += x*w\n",
    "    end\n",
    "    \n",
    "    xs\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `validInterval` ordena las cotas de una variable e intersecta este intervalo con $[0,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function validInterval(a,b)\n",
    "    lo = min(a, b)\n",
    "    hi = max(a, b)\n",
    "    (lo>0.0?lo:0.0, hi<1.0?hi:1.0)\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `randInterval` regresa un número aleatorio en el intervalo dado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function randInterval(lo, hi)\n",
    "    rand()*(hi-lo)+lo\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function sendInputBig(xs, ws)\n",
    "    sendInput(Array{BigFloat,2}(xs), Array{BigFloat,1}(ws))\n",
    "end\n",
    "function sendOutputBig(y, ws)\n",
    "    sendOutput(BigFloat(y), Array{BigFloat, 1}(ws))\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function floatTest(n, wlo, whi, r)\n",
    "    times  = zeros(r)\n",
    "    errors = zeros(r)\n",
    "    \n",
    "    for i in 1:r\n",
    "        xs = rand(n)'\n",
    "        ws = (whi-wlo)*rand(n)+wlo\n",
    "        y  = sendInput(xs, ws)\n",
    "        \n",
    "        tic()\n",
    "        new_xs = sendOutput(y, ws)\n",
    "        time = toq();\n",
    "        times[i] = time\n",
    "        \n",
    "        new_y = sendInput(new_xs, ws)\n",
    "        err = abs(y-new_y)\n",
    "        errors[i] = err\n",
    "    end\n",
    "    \n",
    "    println(\"===================================================\")\n",
    "    println(\"elapsed time mean     = $(mean(times))\")\n",
    "    println(\"elapsed time variance = $(var(times))\")\n",
    "    println(\"---------------------------------------------------\")\n",
    "    println(\"error mean     = $(mean(errors))\")\n",
    "    println(\"error variance = $(var(errors))\")\n",
    "    println(\"===================================================\")\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "elapsed time mean     = 7.495880000000001e-7\n",
      "elapsed time variance = 8.841347799499749e-14\n",
      "---------------------------------------------------\n",
      "error mean     = NaN\n",
      "error variance = NaN\n",
      "===================================================\n"
     ]
    }
   ],
   "source": [
    "floatTest(10,-10, 10, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function gmpTest(n, wlo, whi, r)\n",
    "    times  = Array{BigFloat,1}(zeros(r))\n",
    "    errors = Array{BigFloat,1}(zeros(r))\n",
    "    \n",
    "    for i in 1:r\n",
    "        xs = Array{BigFloat,2}(rand(n)')\n",
    "        ws = Array{BigFloat,1}((whi-wlo)*rand(n)+wlo)\n",
    "        y  = sendInputBig(xs, ws)\n",
    "        \n",
    "        tic()\n",
    "        new_xs = sendOutputBig(y, ws)\n",
    "        time = toq();\n",
    "        times[i] = time\n",
    "        \n",
    "        new_y = sendInputBig(new_xs, ws)\n",
    "        err = abs(y-new_y)\n",
    "        errors[i] = err\n",
    "    end\n",
    "    \n",
    "    println(\"===================================================\")\n",
    "    println(\"elapsed time mean     = $(mean(times))\")\n",
    "    println(\"elapsed time variance = $(var(times))\")\n",
    "    println(\"---------------------------------------------------\")\n",
    "    println(\"error mean     = $(mean(errors))\")\n",
    "    println(\"error variance = $(var(errors))\")\n",
    "    println(\"===================================================\")\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "elapsed time mean     = 3.908829886500000006660515694534296926576644182205200195312499999999999999999987e-03\n",
      "elapsed time variance = 8.169388588053804624644775304591166658881801573245043344027311876579042117536044e-06\n",
      "---------------------------------------------------\n",
      "error mean     = 1.757913061892069497885299027963529259237399072767087986471973939701434247028553e-18\n",
      "error variance = 2.067252161607890105223830804867468600629805912180240145356669772000356454485367e-35\n",
      "===================================================\n"
     ]
    }
   ],
   "source": [
    "gmpTest(1000, -1, 1, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mejoras a la implementación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La reversión de la red neuronal se realiza después de haber ajustado los pesos, por lo tanto, en una implementación completa de las redes neuronales alimentadas hacia adelante se pudieran computar una sola vez los valores de $\\delta$, `negWeight` y `posWeight`. El algoritmo entonces se dividiría en dos partes, una para encontrar estos valores teniendo los pesos fijos y otra para calcular los valores de entrada de la red dado un valor de salida.\n",
    "\n",
    "La complejidad del algoritmo no cambia, pero sería un cambio deseable si consideramos que el cómputo se realizará por vada variable de entrada en cada neurona."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trabajo futuro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pesar de plantearse como observaciones generales de las redes neuronales alimentadas hacia adelante, algunas hipótesis planteadas sólo pueden ser argumentadas analizando el caso particular del proceso de una neurona.\n",
    "\n",
    "Una observación que se deja pendiente para el siguiente trabajo es la siguiente:\n",
    "\n",
    "Existe una dependencia entre el proceso revertido de las neuronas de una misma capa al momento de generar valores para $\\mathbf{x}$ ya que este mismo vector se propaga hacia adelante a todas las neuronas de la capa pero con diferentes pesos.\n",
    "\n",
    "Si hay $m$ neuronas en una capa y el vector de entrada de la capa tiene dimensión $n$, entonces se busca generar valores aleatorios en la familia de soluciones de un sistema de $m$ ecuaciones, cada una con $n$ incógnitas. Por lo tanto, el algoritmo deberá ser modificado para que satisfaga todas ellas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototipo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scheme\n",
    "(import (scheme base)\n",
    "\t    (srfi 27))\n",
    "\n",
    "\n",
    ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
    ";; NEURAL NETWORK PROCEDURES ;;\n",
    ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
    "\n",
    ";;; ANN function\n",
    "(define (sigmoid t)\n",
    "  (/ 1 (+ 1 (exp (- t)))))\n",
    "\n",
    ";;; ANN inverse function\n",
    "(define (logit p)\n",
    "  (- (log (- (/ 1 p) 1))))\n",
    "\n",
    ";;; f and g are aliases for sigmoid and logit respectively\n",
    "(define f sigmoid)\n",
    "(define g logit)\n",
    "\n",
    ";;; !! it is necessary that f^{-1} = g\n",
    "\n",
    ";;; this is forward propagation for one neuron\n",
    "(define (send-input f xs ws)\n",
    "  (f (apply + (map * xs ws))))\n",
    "\n",
    ";;; this is backward propagation for one neuron\n",
    "(define (send-output g y ws)\n",
    "  (define (get-random-solution s w0 wrest)\n",
    "    (define (loop known-xs known-ws unknown-ws)\n",
    "      (if (no-more? unknown-ws)\n",
    "\t  '()\n",
    "\t  (let* ((w  (first unknown-ws))\n",
    "\t\t (ks (map * known-xs known-ws))\n",
    "\t\t (+s (map cut-neg unknown-ws))\n",
    "\t\t (-s (map cut-pos unknown-ws))\n",
    "\t\t (ba (/ (apply - `(,s ,w0 ,@ks ,@+s)) w))\n",
    "\t\t (bb (/ (apply - `(,s ,w0 ,@ks ,@-s)) w))\n",
    "\t\t (in (interval-cut ba bb))\n",
    "\t\t (x (random-value in)))\n",
    "\t    (cons x (loop (cons x known-xs)\n",
    "\t\t\t  (cons w known-ws)\n",
    "\t\t\t  (rest unknown-ws))))))\n",
    "    (loop '() '() wrest))\n",
    "  (cons 1 (get-random-solution (g y) (car ws) (cdr ws))))\n",
    "\n",
    "\n",
    ";;;;;;;;;;;;;;;;;;;;;;;;;;\n",
    ";; AUXILIARY PROCEDURES ;;\n",
    ";;;;;;;;;;;;;;;;;;;;;;;;;;\n",
    "\n",
    "(define (unitary-intersect lo hi)\n",
    "  (cons (if (> lo 0) lo 0)\n",
    "\t(if (< hi 1) hi 1)))\n",
    "\n",
    "(define (random-value interval)\n",
    "  (let ((lo (car interval))\n",
    "\t(hi (cdr interval)))\n",
    "    (+ lo (* (random-real) (- hi lo)))))\n",
    "\n",
    "(define (no-more? xs)\n",
    "  (null? xs))\n",
    "\n",
    "(define (first xs)\n",
    "  (car xs))\n",
    "\n",
    "(define (rest xs)\n",
    "  (cdr xs))\n",
    "\n",
    "(define (cut-neg w)\n",
    "  (if (< w 0) 0 w))\n",
    "\n",
    "(define (cut-pos w)\n",
    "  (if (< w 0) w 0))\n",
    "\n",
    "(define (interval-cut a b)\n",
    "  (unitary-intersect (min a b) (max a b)))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.0",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
