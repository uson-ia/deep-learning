{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neuronales Recurrentes\n",
    "\n",
    "En este tutorial vamos a ver como entrenar una red neuronal recurrente modelando lenguaje. La objetivo es obtener un modelo probabilista en que se le asignan probabilidades a oraciones.\n",
    "\n",
    "Para esto vamos a utilizar los datos de Penn Tree Bank (PTB).\n",
    "\n",
    "## Archivos para este tutorial\n",
    "\n",
    "Este tutorial referencia los siguientes archivos de **models/rnn/ptb**:\n",
    "\n",
    "* ptb_word_lm.py : el cual es para entrenar el modelo del lenguaje.\n",
    "* reader.py : Se utiliza para leer los datos.\n",
    "\n",
    "## Descargar y preparar los datos\n",
    "\n",
    "Los datos que ocupamos se encuentran en esta direccion: http://www.fit.vutbr.cz/~imikolov/rnnl/simple-examples.tgz\n",
    "\n",
    "Los datos ya estan procesados y contiene 10000 diferentes palabras.\n",
    "\n",
    "# El Modelo\n",
    "\n",
    "## LSTM\n",
    "\n",
    "La base de este modelo consiste en la celda LSTM que procesa una palabra a la vez y calcula la probabilidad de la continuacion de esa oracion.\n",
    "\n",
    "El estado de la memoria empieza en cero y se va actualizando conforme se lee cada palabra.\n",
    "\n",
    "El pseudocodigo es el siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "lstm = rnn_cell.BasicLSTMCell(lstm_size)\n",
    "# Initial state of the LSTM memory.\n",
    "state = tf.zeros([batch_size, lstm.state_size])\n",
    "\n",
    "loss = 0.0\n",
    "for current_batch_of_words in words_in_dataset:\n",
    "    # The value of state is updated after processing each batch of words.\n",
    "    output, state = lstm(current_batch_of_words, state)\n",
    "\n",
    "    # The LSTM output can be used to make next word predictions\n",
    "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "    probabilities = tf.nn.softmax(logits)\n",
    "    loss += loss_function(probabilities, target_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation Truncada\n",
    "\n",
    "En orden de hacel el proceso de aprendizaje trazeable, es comun truncar la gradiente del backpropagation a un numero fijo (**num_steps**).\n",
    "\n",
    "A continuacion una version simplificada de la creacion de grafo para una backpropagation truncada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Placeholder for the inputs in a given iteration.\n",
    "words = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "\n",
    "lstm = rnn_cell.BasicLSTMCell(lstm_size)\n",
    "# Initial state of the LSTM memory.\n",
    "initial_state = state = tf.zeros([batch_size, lstm.state_size])\n",
    "\n",
    "for i in range(len(num_steps)):\n",
    "    # The value of state is updated after processing each batch of words.\n",
    "    output, state = lstm(words[:, i], state)\n",
    "\n",
    "    # The rest of the code.\n",
    "    # ...\n",
    "\n",
    "final_state = state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y asi es como se implementara una iteracion sobre todo los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A numpy array holding the state of LSTM after each batch of words.\n",
    "numpy_state = initial_state.eval()\n",
    "total_loss = 0.0\n",
    "for current_batch_of_words in words_in_dataset:\n",
    "    numpy_state, current_loss = session.run([final_state, loss],\n",
    "        # Initialize the LSTM state from the previous iteration.\n",
    "        feed_dict={initial_state: numpy_state, words: current_batch_of_words})\n",
    "    total_loss += current_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputs\n",
    "\n",
    "La id de las palabras se les dara una densa representacion antes de darsela al LSTM. Esto permite al modelo a representar eficientemente el conocimiento de las palabras. Y tambien es facil de escribir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# embedding_matrix is a tensor of shape [vocabulary_size, embedding size]\n",
    "word_embeddings = tf.nn.embedding_lookup(embedding_matrix, word_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matriz va a ser inicializada de forma aleatoria y el modelo va a aprender a difereciar el significado de las palabras solo viendo la informacion.\n",
    "\n",
    "## Funcion de perdida\n",
    "\n",
    "Queremos minimizar la probabilidad promedio negativa de la palabra objetivo.\n",
    "Esto no es muy dificil de implementar pero la funcion **sequence_loss_by_example** esta disponible, asi que podemos utilizarla.\n",
    "\n",
    "## Utilizando multiples LSTMs\n",
    "\n",
    "Para dar a nuestro modelo mas poder expresivo, podemos agregar multiples capas de LSTM a los datos procesados.\n",
    "\n",
    "LLamamos a esto **MultiRNNCell**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lstm = rnn_cell.BasicLSTMCell(lstm_size)\n",
    "stacked_lstm = rnn_cell.MultiRNNCell([lstm] * number_of_layers)\n",
    "\n",
    "initial_state = state = stacked_lstm.zero_state(batch_size, tf.float32)\n",
    "for i in range(len(num_steps)):\n",
    "    # The value of state is updated after processing each batch of words.\n",
    "    output, state = stacked_lstm(words[:, i], state)\n",
    "\n",
    "    # The rest of the code.\n",
    "    # ...\n",
    "\n",
    "final_state = state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correr el Codigo\n",
    "\n",
    "Asumo que ya has instalado via pip la reposteria de tensorflow. \n",
    "\n",
    "Despues escribe esto LdC: **cd tensorflow/models/rnn/ptb python ptb_word_lm --data_path=/tmp/simple-examples/data/ --model small**\n",
    "\n",
    "Hay 3 configuraciones soportadas por el modelo, en este tutorial usaremos el small. la diferencia es el tama√±o y los parametros que se usan para entrenar.\n",
    "\n",
    "Entre mas grande el modelo mejor deberian de ser los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
